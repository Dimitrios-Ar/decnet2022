
 Average loss:  0.3303748071193695  --- Iterations:  8  --- Epochs:  1
/home/robotlabx/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
 Average loss:  0.24800580739974976  --- Iterations:  16  --- Epochs:  1
 Average loss:  0.1931809037923813  --- Iterations:  24  --- Epochs:  1
 Average loss:  0.15744584798812866  --- Iterations:  32  --- Epochs:  1
 Average loss:  0.1318080574274063  --- Iterations:  40  --- Epochs:  1
 Average loss:  0.11358430981636047  --- Iterations:  48  --- Epochs:  1
 Average loss:  0.10013444721698761  --- Iterations:  56  --- Epochs:  1
 Average loss:  0.08969134837388992  --- Iterations:  64  --- Epochs:  1
 Average loss:  0.08129272609949112  --- Iterations:  72  --- Epochs:  1
 Average loss:  0.07449354976415634  --- Iterations:  80  --- Epochs:  1
 Average loss:  0.06889594346284866  --- Iterations:  88  --- Epochs:  1
 Average loss:  0.06401654332876205  --- Iterations:  96  --- Epochs:  1
 Average loss:  0.059796955436468124  --- Iterations:  104  --- Epochs:  1
 Average loss:  0.056258488446474075  --- Iterations:  112  --- Epochs:  1
 Average loss:  0.053084321320056915  --- Iterations:  120  --- Epochs:  1
 Average loss:  0.05024268850684166  --- Iterations:  128  --- Epochs:  1
 Average loss:  0.04770730435848236  --- Iterations:  136  --- Epochs:  1
 Average loss:  0.045547064393758774  --- Iterations:  144  --- Epochs:  1
 Average loss:  0.04360660910606384  --- Iterations:  152  --- Epochs:  1
 Average loss:  0.04179113730788231  --- Iterations:  160  --- Epochs:  1
 Average loss:  0.04015575349330902  --- Iterations:  168  --- Epochs:  1
 Average loss:  0.038609761744737625  --- Iterations:  176  --- Epochs:  1
 Average loss:  0.03722716495394707  --- Iterations:  184  --- Epochs:  1
 Average loss:  0.03590325266122818  --- Iterations:  192  --- Epochs:  1
 Average loss:  0.03469586372375488  --- Iterations:  200  --- Epochs:  1
 Average loss:  0.0335889533162117  --- Iterations:  208  --- Epochs:  1
 Average loss:  0.03259861841797829  --- Iterations:  216  --- Epochs:  1
 Average loss:  0.031589534133672714  --- Iterations:  224  --- Epochs:  1
 Average loss:  0.03069276548922062  --- Iterations:  232  --- Epochs:  1
 Average loss:  0.029871493577957153  --- Iterations:  240  --- Epochs:  1
 Average loss:  0.02906075306236744  --- Iterations:  248  --- Epochs:  1
 Average loss:  0.02827676571905613  --- Iterations:  256  --- Epochs:  1
 Average loss:  0.027543572708964348  --- Iterations:  264  --- Epochs:  1
 Average loss:  0.026833536103367805  --- Iterations:  272  --- Epochs:  1
 Average loss:  0.026198633015155792  --- Iterations:  280  --- Epochs:  1
 Average loss:  0.025578472763299942  --- Iterations:  288  --- Epochs:  1
 Average loss:  0.02501736208796501  --- Iterations:  296  --- Epochs:  1
 Average loss:  0.024454839527606964  --- Iterations:  304  --- Epochs:  1
 Average loss:  0.023946890607476234  --- Iterations:  312  --- Epochs:  1
 Average loss:  0.02346961200237274  --- Iterations:  320  --- Epochs:  1
 Average loss:  0.02298213355243206  --- Iterations:  328  --- Epochs:  1
 Average loss:  0.022510884329676628  --- Iterations:  336  --- Epochs:  1
 Average loss:  0.02208060771226883  --- Iterations:  344  --- Epochs:  1
 Average loss:  0.021652037277817726  --- Iterations:  352  --- Epochs:  1
 Average loss:  0.021246720105409622  --- Iterations:  360  --- Epochs:  1
 Average loss:  0.020864315330982208  --- Iterations:  368  --- Epochs:  1
 Average loss:  0.02047826535999775  --- Iterations:  376  --- Epochs:  1
 Average loss:  0.02010994404554367  --- Iterations:  384  --- Epochs:  1
 Average loss:  0.019757181406021118  --- Iterations:  392  --- Epochs:  1
 Average loss:  0.019434720277786255  --- Iterations:  400  --- Epochs:  1
 Average loss:  0.019113801419734955  --- Iterations:  408  --- Epochs:  1
 Average loss:  0.018813302740454674  --- Iterations:  416  --- Epochs:  1
 Average loss:  0.018537746742367744  --- Iterations:  424  --- Epochs:  1
 Average loss:  0.018258769065141678  --- Iterations:  432  --- Epochs:  1
 Average loss:  0.017988495528697968  --- Iterations:  440  --- Epochs:  1
 Average loss:  0.017743000760674477  --- Iterations:  448  --- Epochs:  1
 Average loss:  0.01749606244266033  --- Iterations:  456  --- Epochs:  1
 Average loss:  0.017233796417713165  --- Iterations:  464  --- Epochs:  1
 Average loss:  0.016997503116726875  --- Iterations:  472  --- Epochs:  1
 Average loss:  0.016749590635299683  --- Iterations:  480  --- Epochs:  1
 Average loss:  0.016520027071237564  --- Iterations:  488  --- Epochs:  1
 Average loss:  0.016297459602355957  --- Iterations:  496  --- Epochs:  1
 Average loss:  0.01607816480100155  --- Iterations:  504  --- Epochs:  1
 Average loss:  0.015869341790676117  --- Iterations:  512  --- Epochs:  1
 Average loss:  0.015658536925911903  --- Iterations:  520  --- Epochs:  1
 Average loss:  0.015465951524674892  --- Iterations:  528  --- Epochs:  1
 Average loss:  0.015276330523192883  --- Iterations:  536  --- Epochs:  1
 Average loss:  0.01508807297796011  --- Iterations:  544  --- Epochs:  1
 Average loss:  0.014917545020580292  --- Iterations:  552  --- Epochs:  1
 Average loss:  0.014743144623935223  --- Iterations:  560  --- Epochs:  1
 Average loss:  0.014573421329259872  --- Iterations:  568  --- Epochs:  1
 Average loss:  0.014412909746170044  --- Iterations:  576  --- Epochs:  1
 Average loss:  0.014244234189391136  --- Iterations:  584  --- Epochs:  1
 Average loss:  0.014101328328251839  --- Iterations:  592  --- Epochs:  1
 Average loss:  0.013955374248325825  --- Iterations:  600  --- Epochs:  1
 Average loss:  0.013812670484185219  --- Iterations:  608  --- Epochs:  1
 Average loss:  0.013672382570803165  --- Iterations:  616  --- Epochs:  1
 Average loss:  0.01353639829903841  --- Iterations:  624  --- Epochs:  1
 Average loss:  0.013398058712482452  --- Iterations:  632  --- Epochs:  1
 Average loss:  0.013257910497486591  --- Iterations:  640  --- Epochs:  1
 Average loss:  0.013133885338902473  --- Iterations:  648  --- Epochs:  1
 Average loss:  0.013014071621000767  --- Iterations:  656  --- Epochs:  1
 Average loss:  0.012896745465695858  --- Iterations:  664  --- Epochs:  1
 Average loss:  0.01277502253651619  --- Iterations:  672  --- Epochs:  1
 Average loss:  0.012660200707614422  --- Iterations:  680  --- Epochs:  1
 Average loss:  0.012557602487504482  --- Iterations:  688  --- Epochs:  1
 Average loss:  0.01243608444929123  --- Iterations:  696  --- Epochs:  1
 Average loss:  0.012323938310146332  --- Iterations:  704  --- Epochs:  1
 Average loss:  0.012224378064274788  --- Iterations:  712  --- Epochs:  1
 Average loss:  0.012124652974307537  --- Iterations:  720  --- Epochs:  1
 Average loss:  0.012021100148558617  --- Iterations:  728  --- Epochs:  1
 Average loss:  0.011910205706954002  --- Iterations:  736  --- Epochs:  1
 Average loss:  0.01181013137102127  --- Iterations:  744  --- Epochs:  1
 Average loss:  0.011709528975188732  --- Iterations:  752  --- Epochs:  1
 Average loss:  0.011613164097070694  --- Iterations:  760  --- Epochs:  1
 Average loss:  0.01151041965931654  --- Iterations:  768  --- Epochs:  1
 Average loss:  0.011418418027460575  --- Iterations:  776  --- Epochs:  1
 Average loss:  0.011319046840071678  --- Iterations:  784  --- Epochs:  1
 Average loss:  0.011235418729484081  --- Iterations:  792  --- Epochs:  1
 Average loss:  0.011139770038425922  --- Iterations:  800  --- Epochs:  1
 Average loss:  0.011051010340452194  --- Iterations:  808  --- Epochs:  1
 Average loss:  0.010962397791445255  --- Iterations:  816  --- Epochs:  1
 Average loss:  0.010881830006837845  --- Iterations:  824  --- Epochs:  1
 Average loss:  0.010798323899507523  --- Iterations:  832  --- Epochs:  1
 Average loss:  0.010721325874328613  --- Iterations:  840  --- Epochs:  1
 Average loss:  0.010642114095389843  --- Iterations:  848  --- Epochs:  1
 Average loss:  0.010561774484813213  --- Iterations:  856  --- Epochs:  1
 Average loss:  0.010499699972569942  --- Iterations:  864  --- Epochs:  1
 Average loss:  0.010427466593682766  --- Iterations:  872  --- Epochs:  1
 Average loss:  0.010353491641581059  --- Iterations:  880  --- Epochs:  1
 Average loss:  0.010282577015459538  --- Iterations:  888  --- Epochs:  1
 Average loss:  0.01020743977278471  --- Iterations:  896  --- Epochs:  1
 Average loss:  0.01014138013124466  --- Iterations:  904  --- Epochs:  1
 Average loss:  0.010070283897221088  --- Iterations:  912  --- Epochs:  1
 Average loss:  0.010001588612794876  --- Iterations:  920  --- Epochs:  1
 Average loss:  0.009927003644406796  --- Iterations:  928  --- Epochs:  1
 Average loss:  0.009856624528765678  --- Iterations:  936  --- Epochs:  1
 Average loss:  0.009794313460588455  --- Iterations:  944  --- Epochs:  1
 Average loss:  0.009725629352033138  --- Iterations:  952  --- Epochs:  1
 Average loss:  0.009664572775363922  --- Iterations:  960  --- Epochs:  1
 Average loss:  0.00960409827530384  --- Iterations:  968  --- Epochs:  1
 Average loss:  0.009548270143568516  --- Iterations:  976  --- Epochs:  1
 Average loss:  0.009490247815847397  --- Iterations:  984  --- Epochs:  1
 Average loss:  0.009439270943403244  --- Iterations:  992  --- Epochs:  1
 Average loss:  0.009380117058753967  --- Iterations:  1000  --- Epochs:  1
 Average loss:  0.009323877282440662  --- Iterations:  1008  --- Epochs:  1
 Average loss:  0.009284136816859245  --- Iterations:  1016  --- Epochs:  1
 Average loss:  0.00923036690801382  --- Iterations:  1024  --- Epochs:  1
 Average loss:  0.009174803271889687  --- Iterations:  1032  --- Epochs:  1
 Average loss:  0.00912129133939743  --- Iterations:  1040  --- Epochs:  1
 Average loss:  0.009065289981663227  --- Iterations:  1048  --- Epochs:  1
 Average loss:  0.009012170135974884  --- Iterations:  1056  --- Epochs:  1
 Average loss:  0.0089596938341856  --- Iterations:  1064  --- Epochs:  1
 Average loss:  0.008907543495297432  --- Iterations:  1072  --- Epochs:  1
 Average loss:  0.008854546584188938  --- Iterations:  1080  --- Epochs:  1
 Average loss:  0.008799982257187366  --- Iterations:  1088  --- Epochs:  1
 Average loss:  0.008761173114180565  --- Iterations:  1096  --- Epochs:  1
 Average loss:  0.008711548522114754  --- Iterations:  1104  --- Epochs:  1
 Average loss:  0.008662154898047447  --- Iterations:  1112  --- Epochs:  1
 Average loss:  0.008616423234343529  --- Iterations:  1120  --- Epochs:  1
 Average loss:  0.008569018915295601  --- Iterations:  1128  --- Epochs:  1
 Average loss:  0.008521544747054577  --- Iterations:  1136  --- Epochs:  1
 Average loss:  0.00848117470741272  --- Iterations:  1144  --- Epochs:  1
 Average loss:  0.00843537226319313  --- Iterations:  1152  --- Epochs:  1
 Average loss:  0.008394183591008186  --- Iterations:  1160  --- Epochs:  1
 Average loss:  0.008349836803972721  --- Iterations:  1168  --- Epochs:  1
 Average loss:  0.00830487348139286  --- Iterations:  1176  --- Epochs:  1
 Average loss:  0.008263777010142803  --- Iterations:  1184  --- Epochs:  1
 Average loss:  0.008218432776629925  --- Iterations:  1192  --- Epochs:  1
 Average loss:  0.00817510299384594  --- Iterations:  1200  --- Epochs:  1
 Average loss:  0.008135747164487839  --- Iterations:  1208  --- Epochs:  1
 Average loss:  0.008099253289401531  --- Iterations:  1216  --- Epochs:  1
 Average loss:  0.00805911049246788  --- Iterations:  1224  --- Epochs:  1
 Average loss:  0.008022547699511051  --- Iterations:  1232  --- Epochs:  1
 Average loss:  0.007981727831065655  --- Iterations:  1240  --- Epochs:  1
 Average loss:  0.00794337410479784  --- Iterations:  1248  --- Epochs:  1
 Average loss:  0.007899560034275055  --- Iterations:  1256  --- Epochs:  1
 Average loss:  0.007865805178880692  --- Iterations:  1264  --- Epochs:  1
 Average loss:  0.007828396745026112  --- Iterations:  1272  --- Epochs:  1
 Average loss:  0.007797163911163807  --- Iterations:  1280  --- Epochs:  1
 Average loss:  0.007758391555398703  --- Iterations:  1288  --- Epochs:  1
 Average loss:  0.00772231537848711  --- Iterations:  1296  --- Epochs:  1
 Average loss:  0.00768780242651701  --- Iterations:  1304  --- Epochs:  1
 Average loss:  0.007652906700968742  --- Iterations:  1312  --- Epochs:  1
 Average loss:  0.007617244962602854  --- Iterations:  1320  --- Epochs:  1
 Average loss:  0.007584348786622286  --- Iterations:  1328  --- Epochs:  1
 Average loss:  0.007551886141300201  --- Iterations:  1336  --- Epochs:  1
 Average loss:  0.007519422564655542  --- Iterations:  1344  --- Epochs:  1
 Average loss:  0.007488564122468233  --- Iterations:  1352  --- Epochs:  1
 Average loss:  0.0074562132358551025  --- Iterations:  1360  --- Epochs:  1
 Average loss:  0.00742679089307785  --- Iterations:  1368  --- Epochs:  1
 Average loss:  0.007397268433123827  --- Iterations:  1376  --- Epochs:  1
 Average loss:  0.007367000449448824  --- Iterations:  1384  --- Epochs:  1
 Average loss:  0.007333698682487011  --- Iterations:  1392  --- Epochs:  1
 Average loss:  0.007300568278878927  --- Iterations:  1400  --- Epochs:  1
 Average loss:  0.007270796224474907  --- Iterations:  1408  --- Epochs:  1
 Average loss:  0.007245760876685381  --- Iterations:  1416  --- Epochs:  1
 Average loss:  0.007216769736260176  --- Iterations:  1424  --- Epochs:  1
 Average loss:  0.007185895927250385  --- Iterations:  1432  --- Epochs:  1
 Average loss:  0.0071590063162148  --- Iterations:  1440  --- Epochs:  1
 Average loss:  0.007127237971872091  --- Iterations:  1448  --- Epochs:  1
 Average loss:  0.007100981194525957  --- Iterations:  1456  --- Epochs:  1
 Average loss:  0.00707768090069294  --- Iterations:  1464  --- Epochs:  1
 Average loss:  0.00704672746360302  --- Iterations:  1472  --- Epochs:  1
 Average loss:  0.0070192283019423485  --- Iterations:  1480  --- Epochs:  1
 Average loss:  0.006995393894612789  --- Iterations:  1488  --- Epochs:  1
 Average loss:  0.006966454442590475  --- Iterations:  1496  --- Epochs:  1
 Average loss:  0.006949550937861204  --- Iterations:  1504  --- Epochs:  1
 Average loss:  0.006924412213265896  --- Iterations:  1512  --- Epochs:  1
 Average loss:  0.006898523308336735  --- Iterations:  1520  --- Epochs:  1
 Average loss:  0.006875221151858568  --- Iterations:  1528  --- Epochs:  1
 Average loss:  0.006847083102911711  --- Iterations:  1536  --- Epochs:  1
 Average loss:  0.006821074057370424  --- Iterations:  1544  --- Epochs:  1
 Average loss:  0.006799283437430859  --- Iterations:  1552  --- Epochs:  1
 Average loss:  0.006772889290004969  --- Iterations:  1560  --- Epochs:  1
 Average loss:  0.006746863480657339  --- Iterations:  1568  --- Epochs:  1
 Average loss:  0.0067199417389929295  --- Iterations:  1576  --- Epochs:  1
 Average loss:  0.006695325952023268  --- Iterations:  1584  --- Epochs:  1
 Average loss:  0.006671390496194363  --- Iterations:  1592  --- Epochs:  1
 Average loss:  0.006645657122135162  --- Iterations:  1600  --- Epochs:  1
 Average loss:  0.0066179982386529446  --- Iterations:  1608  --- Epochs:  1
 Average loss:  0.00659269280731678  --- Iterations:  1616  --- Epochs:  1
 Average loss:  0.006569022312760353  --- Iterations:  1624  --- Epochs:  1
 Average loss:  0.006546459160745144  --- Iterations:  1632  --- Epochs:  1
 Average loss:  0.006521882489323616  --- Iterations:  1640  --- Epochs:  1
 Average loss:  0.00649781059473753  --- Iterations:  1648  --- Epochs:  1
 Average loss:  0.006475177127867937  --- Iterations:  1656  --- Epochs:  1
 Average loss:  0.006453785579651594  --- Iterations:  1664  --- Epochs:  1
 Average loss:  0.0064321840181946754  --- Iterations:  1672  --- Epochs:  1
 Average loss:  0.0064105563797056675  --- Iterations:  1680  --- Epochs:  1
 Average loss:  0.006391793489456177  --- Iterations:  1688  --- Epochs:  1
 Average loss:  0.006372231990098953  --- Iterations:  1696  --- Epochs:  1
 Average loss:  0.006354441400617361  --- Iterations:  1704  --- Epochs:  1
 Average loss:  0.006334161851555109  --- Iterations:  1712  --- Epochs:  1
 Average loss:  0.006313105579465628  --- Iterations:  1720  --- Epochs:  1
 Average loss:  0.006292256992310286  --- Iterations:  1728  --- Epochs:  1
 Average loss:  0.00627166498452425  --- Iterations:  1736  --- Epochs:  1
 Average loss:  0.006252007558941841  --- Iterations:  1744  --- Epochs:  1
 Average loss:  0.006233908701688051  --- Iterations:  1752  --- Epochs:  1
 Average loss:  0.006214271765202284  --- Iterations:  1760  --- Epochs:  1
 Average loss:  0.006193667184561491  --- Iterations:  1768  --- Epochs:  1
 Average loss:  0.006173418369144201  --- Iterations:  1776  --- Epochs:  1
 Average loss:  0.006155733484774828  --- Iterations:  1784  --- Epochs:  1
 Average loss:  0.006135937292128801  --- Iterations:  1792  --- Epochs:  1
 Average loss:  0.006114852149039507  --- Iterations:  1800  --- Epochs:  1
 Average loss:  0.006095495540648699  --- Iterations:  1808  --- Epochs:  1
 Average loss:  0.00607676524668932  --- Iterations:  1816  --- Epochs:  1
 Average loss:  0.006054566707462072  --- Iterations:  1824  --- Epochs:  1
 Average loss:  0.006039397791028023  --- Iterations:  1832  --- Epochs:  1
 Average loss:  0.006022242829203606  --- Iterations:  1840  --- Epochs:  1
 Average loss:  0.0060066464357078075  --- Iterations:  1848  --- Epochs:  1
 Average loss:  0.005987273994833231  --- Iterations:  1856  --- Epochs:  1
 Average loss:  0.005967620760202408  --- Iterations:  1864  --- Epochs:  1
 Average loss:  0.005949041340500116  --- Iterations:  1872  --- Epochs:  1
 Average loss:  0.005931027699261904  --- Iterations:  1880  --- Epochs:  1
 Average loss:  0.0059132627211511135  --- Iterations:  1888  --- Epochs:  1
 Average loss:  0.005899027455598116  --- Iterations:  1896  --- Epochs:  1
 Average loss:  0.00588253466412425  --- Iterations:  1904  --- Epochs:  1
 Average loss:  0.005864312872290611  --- Iterations:  1912  --- Epochs:  1
 Average loss:  0.005847807042300701  --- Iterations:  1920  --- Epochs:  1
 Average loss:  0.00582992983981967  --- Iterations:  1928  --- Epochs:  1
 Average loss:  0.005812075454741716  --- Iterations:  1936  --- Epochs:  1
 Average loss:  0.005796163808554411  --- Iterations:  1944  --- Epochs:  1
 Average loss:  0.005779743660241365  --- Iterations:  1952  --- Epochs:  1
 Average loss:  0.005765809677541256  --- Iterations:  1960  --- Epochs:  1
 Average loss:  0.005751103162765503  --- Iterations:  1968  --- Epochs:  1
 Average loss:  0.005735443904995918  --- Iterations:  1976  --- Epochs:  1
 Average loss:  0.005716826766729355  --- Iterations:  1984  --- Epochs:  1
 Average loss:  0.0056995912455022335  --- Iterations:  1992  --- Epochs:  1
 Average loss:  0.005685847718268633  --- Iterations:  2000  --- Epochs:  1
Starting evaluation
Traceback (most recent call last):
  File "enet_sanity.py", line 428, in <module>
    st1_pred, st2_pred, pred = model(batch_data)
  File "/home/robotlabx/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/robotlabx/Desktop/DTAI/decnet2022/src/models/penet2021/penet2021_model.py", line 110, in forward
    d_s2, vm_s2 = self.sparsepooling(d, valid_mask)
  File "/home/robotlabx/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/robotlabx/Desktop/DTAI/decnet2022/src/models/penet2021/penet2021_basic.py", line 149, in forward
    d = - self.pooling(encode_d)
  File "/home/robotlabx/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/robotlabx/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    self.return_indices)
  File "/home/robotlabx/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/torch/_jit_internal.py", line 405, in fn
    return if_false(*args, **kwargs)
  File "/home/robotlabx/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/torch/nn/functional.py", line 718, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: "max_pool2d_with_indices_out_cuda_frame" not implemented for 'Int'